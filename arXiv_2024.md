*Wei Tang, Yixin Cao, Yang Deng, Jiahao Ying, Bo Wang, Yizhe Yang, Yuyue Zhao, Qi Zhang, Xuanjing Huang, Yugang Jiang, Yong Liao*, **“EvoWiki: Evaluating LLMs on evolving knowledge”** [[HOME Page]](https://arxiv.org/abs/2412.13582) [[PDF]](https://arxiv.org/pdf/2412.13582)



*Xiaobao Wu, Liangming Pan, Yuxi Xie, Ruiwen Zhou, Shuai Zhao, Yubo Ma, Mingzhe Du, Rui Mao, Anh Tuan Luu, William Yang Wang*, **“AntiLeak-Bench: Preventing data contamination by automatically constructing benchmarks with updated real-world knowledge”** [[HOME Page]](https://arxiv.org/abs/2412.13670) [[PDF]](https://arxiv.org/pdf/2412.13670)



*Aidar Myrzakhan, Sondos Mahmoud Bsharat, Zhiqiang Shen*, **“Open-LLM-Leaderboard: From multi-choice to open-style questions for llms evaluation, benchmark, and arena”** [[HOME Page]](https://arxiv.org/abs/2406.07545) [[PDF]](https://arxiv.org/pdf/2406.07545)



*Xueguang Ma, Shengyao Zhuang, Bevan Koopman, Guido Zuccon, Wenhu Chen, Jimmy Lin*, **“VISA: Retrieval augmented generation with visual source attribution”** [[HOME Page]](https://arxiv.org/abs/2412.14457v1) [[PDF]](https://arxiv.org/pdf/2412.14457v1)



*Jaemin Cho, Debanjan Mahata, Ozan Irsoy, Yujie He, Mohit Bansal*, **“M3DocRAG: Multi-modal retrieval is what you need for multi-page multi-document understanding”** [[HOME Page]](https://arxiv.org/abs/2411.04952) [[PDF]](https://arxiv.org/pdf/2411.04952)



*Zora Zhiruo Wang, Akari Asai, Xinyan Velocity Yu, Frank F. Xu, Yiqing Xie, Graham Neubig, Daniel Fried*, **“CodeRAG-Bench: Can retrieval augment code generation?”** [[HOME Page]](https://arxiv.org/abs/2406.14497) [[PDF]](https://arxiv.org/pdf/2406.14497)



*Wangtao Sun, Chenxiang Zhang, XueYou Zhang, Xuanqing Yu, Ziyang Huang, Pei Chen, Haotian Xu, Shizhu He, Jun Zhao, Kang Liu*, **“Beyond instruction following: Evaluating inferential rule following of large language models,”** [[HOME Page]](https://arxiv.org/abs/2407.08440) [[PDF]](https://arxiv.org/pdf/2407.08440)



*Fei Wang, Xingyu Fu, James Y. Huang, Zekun Li, Qin Liu, Xiaogeng Liu, Mingyu Derek Ma, Nan Xu, Wenxuan Zhou, Kai Zhang, Tianyi Lorena Yan, Wenjie Jacky Mo, Hsiang-Hui Liu, Pan Lu, Chunyuan Li, Chaowei Xiao, Kai-Wei Chang, Dan Roth, Sheng Zhang, Hoifung Poon, Muhao Chen*, **“MuirBench: A comprehensive benchmark for robust multi-image understanding”** [[HOME Page]](https://arxiv.org/abs/2406.09411) [[PDF]](https://arxiv.org/pdf/2406.09411)



*Zhuoran Jin, Hongbang Yuan, Tianyi Men, Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao*, **“RAG-RewardBench: Benchmarking reward models in retrieval augmented generation for preference alignment”** [[HOME Page]](https://arxiv.org/abs/2412.13746) [[PDF]](https://arxiv.org/pdf/2412.13746)



*Tinghui Zhu, Qin Liu, Fei Wang, Zhengzhong Tu, Muhao Chen*, **“Unraveling cross-modality knowledge conflicts in large vision-language models”** [[HOME Page]](https://arxiv.org/abs/2410.03659) [[PDF]](https://arxiv.org/pdf/2410.03659)



*Ruochen Zhao, Wenxuan Zhang, Yew Ken Chia, Weiwen Xu, Deli Zhao, Lidong Bing*, **“Auto arena of llms: Automating llm evaluations with agent peer-battles and committee discussions”** [[HOME Page]](https://arxiv.org/abs/2405.20267) [[PDF]](https://arxiv.org/pdf/2405.20267)



*Zhumin Chu, Qingyao Ai, Yiteng Tu, Haitao Li, Yiqun Liu*, **“PRE: A peer review based large language model evaluator”** [[HOME Page]](https://arxiv.org/abs/2401.15641) [[PDF]](https://arxiv.org/pdf/2401.15641)



*Bhrij Patel, Souradip Chakraborty, Wesley A. Suttle, Mengdi Wang, Amrit Singh Bedi, Dinesh Manocha*, **“AIME: AI system optimization via multiple LLM evaluators”** [[HOME Page]](https://arxiv.org/abs/2410.03131) [[PDF]](https://arxiv.org/pdf/2410.03131)



*Chaithanya Bandi, Abir Harrasse*, **“Adversarial multi-agent evaluation of large language models through iterative debates”** [[HOME Page]](https://arxiv.org/abs/2410.04663) [PDF](https://arxiv.org/pdf/2410.04663)



*Hui Huang, Yingqi Qu, Xingyuan Bu, Hongli Zhou, Jing Liu, Muyun Yang, Bing Xu, Tiejun Zhao*, **“An empirical study of LLM-as-a-judge for LLM evaluation: Fine-tuned judge models is not a genera substitute for GPT-4”** [[HOME Page]](https://arxiv.org/abs/2403.02839) [[PDF]](https://arxiv.org/pdf/2403.02839)



*Tianhao Wu, Weizhe Yuan, Olga Golovneva, Jing Xu, Yuandong Tian, Jiantao Jiao, Jason Weston, Sainbayar Sukhbaatar*, **“Meta-rewarding language models: Self-improving alignment with LLM-as-a-meta-judge”** [[HOME Page]](https://arxiv.org/abs/2407.19594) [[PDF]](https://arxiv.org/pdf/2407.19594)
